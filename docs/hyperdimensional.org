* Hyperdimensional Computing
** Captured
*** [2020-08-01 Sat]
- [[https://www.researchgate.net/publication/311895162_Compact_and_Interpretable_Dialogue_State_Representation_with_Genetic_Sparse_Distributed_Memory][Compact and Interpretable Dialogue State Representation with Genetic Sparse Distributed Memory]]
  - AU: El Asri, Layla
  - AU: Laroche, Romain
  - AU: Pietquin, Olivier
  - PY: 2017/12/01
  - Abstract
    #+begin_quote
    User satisfaction is often considered as the objective that should
    be achieved by spoken dialogue systems. This is why the reward
    function of Spoken Dialogue Systems (SDS) trained by Reinforcement
    Learning (RL) is often designed to reflect user satisfaction. To
    do so, the state space representation should be based on features
    capturing user satisfaction characteristics such as the mean
    speech recognition confidence score for instance. On the other
    hand, for deployment in industrial systems there is a need for
    state representations that are understandable by system
    engineers. In this article, we propose to represent the state
    space using a Genetic Sparse Distributed Memory. This is a state
    aggregation method computing state prototypes which are selected
    so as to lead to the best linear representation of the value
    function in RL. To do so, previous work on **Genetic Sparse
    Distributed Memory** for classification is adapted to the
    Reinforcement Learning task and a new way of building the
    prototypes is proposed. The approach is tested on a corpus of
    dialogues collected with an appointment scheduling system. The
    results are compared to a grid-based linear parametrisation. It is
    shown that learning is accelerated and made more memory
    efficient. It is also shown that the framework is scalable in that
    it is possible to include many dialogue features in the
    representation, interpret the resulting policy and identify the
    most important dialogue features.
    #+end_quote
- [[https://www.sciencedirect.com/science/article/abs/pii/S0031320397000174][A genetic sparse distributed memory approach to the application of handwritten character recognition]]
  - Kuo-Chin Fan, Yuan-Kai Wang, Taiwan
  - Pattern Recognition Volume 30, Issue 12, December **1997**, Pages 2015-2022
  - Abstract
    #+begin_quote
    Kanerva's Sparse Distributed Memory (SDM) is one of the
    self-organizing neural networks that mimic closely the
    psychological behavior of the human brain. In this paper, a
    Genetic Sparse Distributed Memory (GSDM) model that combines SDM
    with genetic algorithms is proposed. The proposed GSDM model not
    only maintains the advantages of both SDM and genetic algorithms,
    but also has higher memory utilization to improve the recognition
    rate. Its effective performance is also verified by application to
    Optical Character Recognition (OCR). Experimental results reveal
    the feasibility and validity of the proposed model.
    #+end_quote
- [[https://arxiv.org/pdf/1207.5774.pdf][A New Training Algorithm for Kanerva’s Sparse Distributed Memory]]
  - Lou Marvin Caraig
  - 2012
  - Abstract
    #+begin_quote
    The Sparse Distributed Memory proposed by Pentii Kanerva (SDM in
    short) was thought to be a model of human long term memory. The
    architecture of the SDM permits to store binary patterns and to
    retrieve them using partially matching patterns.  However
    Kanerva’s model is especially efficient only in handling random
    data. The purpose of this article is to introduce a new approach
    of training Kanerva’s SDM that can handle efficiently non-random
    data, and to provide it the capability to recognize inverted
    patterns. This approach uses a signal model which is different
    from the one proposed for different purposes by Hely, Willshaw and
    Hayes in [4]. This article additionally suggests a different way
    of creating hard locations in the memory despite the Kanerva’s
    static model.
    #+end_quote
- Anwar and Franklin
  - 2018
    - [[https://slideplayer.com/slide/14536735/][Sparse distributed memory for ‘conscious’ software agents]]
      - [ ] Interesting slides
  - 1999
    - [[https://www.researchgate.net/publication/3810408_Using_genetic_algorithms_for_sparse_distributed_memory_initialization][Using genetic algorithms for sparse distributed memory initialization]]
      - Ashraf Anwar, Dipankar Dasgupta, Stan Franklin
      - Conference Paper IEEE
      - February 1999
      - DOI: 10.1109/CEC.1999.782538 · Source: IEEE Xplore ·
      - Abstract
	#+begin_quote
	We describe the use of genetic algorithms to initialize a set of
	hard locations that constitutes the storage space of Sparse
	Distributed Memory (SDM). SDM is an associative memory technique
	that uses binary spaces, and relies on close memory items tending
	to be clustered together, with some level of abstraction. An
	important factor in the physical implementation of SDM is how many
	hard locations are used, which greatly affects the memory
	capacity. It is also dependent on the dimension of the binary
	space used. For the SDM system to function appropriately, the hard
	locations should be uniformly distributed over the binary
	space. We represented a set of hard locations of SDM as population
	members, and employed GA to search for the best (fittest)
	distribution of hard locations over the vast binary
	space. Accordingly, fitness is based on how far each hard location
	is from all other hard locations, which measures the uniformity of
	the distribution. The preliminary results are very promising, with
	the GA significantly outperforming random initialization used in
	most existing SDM implementations. This use of GA, which is
	similar to the Michigan approach, differs from the standard
	approach in that the object of the search is the entire population
	#+end_quote
- [[https://www.semanticscholar.org/paper/Statistical-Sparse-Distributed-Memory-Prediction-David-Rogers/9cfd59da6778b0d14b18ca689d6c132f53e2de74][Statistical Sparse Distributed Memory Prediction with Kanerva's]]
  - David, Rogers
  - Published 1989
  - Abstract
    #+begin_quote
    A new viewpoint of the processing: performed by Kanerva's sparse
    distributed memory (SDM) is presented. In conditions of nearor
    overcapacity, where the associative-memory behavior of the model
    breaks down, the processing performed by the model can be
    interpreted as that of a statistical predictor. Mathematical
    results are presented which serve as the framework for a new
    statistical viewpoint of sparse distributed memory and for which
    the standard formulation of SDM is a special case. This viewpoint
    suggests possible enhancements to the SDM model, including a
    procedure for improving the predictiveness of the system based on
    Holland's work with 'Genetic Algorithms', and a method for
    improving the capacity of SDM even when used as an associative
    memory.
    #+end_quote
*** [2020-08-01 Sat]
- [[https://www.frontiersin.org/articles/10.3389/frobt.2020.00063/full][Symbolic Representation and Learning With Hyperdimensional Computing]]
  - Front. Robot. AI, 09 June 2020 
  - Original Research Article
  - https://doi.org/10.3389/frobt.2020.00063
  - Abstract
    #+begin_quote
    It has been proposed that machine learning techniques can benefit
    from symbolic representations and reasoning systems. We describe a
    method in which the two can be combined in a natural and direct way
    by use of hyperdimensional vectors and hyperdimensional
    computing. By using **hashing neural networks** to produce binary vector
    representations of images, we show how hyperdimensional vectors can
    be constructed such that vector-symbolic inference arises naturally
    out of their output. We design the **Hyperdimensional Inference Layer**
    (HIL) to facilitate this process and evaluate its performance
    compared to baseline hashing networks. In addition to this, we show
    that separate network outputs can directly be fused at the vector
    symbolic level within HILs to improve performance and robustness of
    the overall model. Furthermore, to the best of our knowledge, this
    is the **first** instance in which **meaningful hyperdimensional
    representations of images** are created on real data, while still
    maintaining hyperdimensionality.
    #+end_quote

** Kanerva, Pentti
*** Computing with High-Dimensional Vectors (papers & talks)
- [[https://twitter.com/johntigue/status/1041255480868265984][twitter:@johntigue:2018-09-16]] links to 2017 Stanford Seminar
  #+BEGIN_QUOTE
  Computing with High-dimensional Vectors Stanford lecture on the 2017
  state of Kanerva's math, SDM (1988) and on.
  #+END_QUOTE

- [[https://www.youtube.com/watch?v=zUCoxhExe0o][Stanford Seminar - Computing with High-Dimensional Vectors]]
  - youtube:stanfordonline
    - Published on Oct 26, 2017
    - P1h
  - This may be the one to start with. The whole presentation is well
    developed and smooth, well, as smooth as an introduction to a
    whole new way of computing can be

- [[https://ieeexplore.ieee.org/abstract/document/8594669/authors][Computing with High-Dimensional Vectors (2018-12 IEEE Design & Test)]]
    #+BEGIN_QUOTE
    Editor's note: The author reviews the principles of
    high-dimensional (HD) computing as a brain-inspired paradigm, with
    variables and operations encoded in vectors with high
    dimensionality (e.g., 10,000). HD computing has been shown to be a
    robust novel approach with promising applications in language and
    biosignal processing. -An Chen, Semiconductor Research
    Corporation.
    #+END_QUOTE

- [[https://www.youtube.com/watch?v=oB_mHCurNCI][Pentti Kanerva: Computing with Hypervectors]]
  - youtube: UC Berkeley Events
    - P34m02s
    - Published on Mar 31, 2016
  - [ ] Slides (not exact match) [[http://web.stanford.edu/class/ee380/Abstracts/171025-slides.pdf][Computing With High-Dimensional Vectors]]
    - stanford:classes:ee380
    - [ ] So stanford URL for slides, for Berkeley talk?
    #+BEGIN_QUOTE
    Pentti Kanerva
      UC Berkeley, Redwood Center for Theoretical Neuroscience
      Stanford, CSLI
      pkanerva@csli.stanfrod.edu
    #+END_QUOTE

*** Kanerva talks
- [ ] [[https://archive.org/search.php?query=pentti%20kanerva][SERP on archive.org]]
- [[https://www.youtube.com/watch?v=zUCoxhExe0o][YouTube(Stanfordonline, 2017, 60m)]]
  #+BEGIN_QUOTE
  in@4m: [in slides] reverse-engineering the brain in the absense of
  an adequate theory of computing is next to impossible

  [[https://www.youtube.com/watch?v=zUCoxhExe0o][in@58m]]: by the way natural language processing is one of the areas
  where I hope that we will really make some important advances
  because now we have a system where we can actually represent
  structure because we can we can in in some sense we could implement
  Lisp
  #+END_QUOTE
- [[https://www.youtube.com/watch?v=oB_mHCurNCI][Pentti Kanerva: Computing with Hypervectors]]
  - UC Berkeley Events
  - Published on Mar 31, 2016
- [[https://archive.org/details/ucbvs298_neural_comp_2008_12_09][Pentti Kanerva: UC Berkeley VS298 - Neural Computation Lecture 2008-12-09]]
- [[https://archive.org/details/Redwood_Center_2014_02_14_Pentti_Kanerva][Pentti Kanerva: Neurocomputing a la von Neumann]] Publication date 2014-02-14
- [[https://archive.org/details/Redwood_Center_2017_06_09_Pentti_Kanerva][Pentti Kanerva: The Brain's Circuits Suggest Computing with High-Dimensional Vectors]] Publication date 2017-06-09
- [[https://youtu.be/zUCoxhExe0o][Stanford Seminar - Computing with High-Dimensional Vectors]]
*** Kanerva papers
- Where to start for newbies
  - Kanerva 2014 (8p)
    - [[http://www.rctn.org/vs265/Kanerva-allerton2014.pdf][Computing with 10,000-Bit Words]]
      - published in IEEE's 2014 52nd Annual Allerton Conference.
      - https://ieeexplore.ieee.org/abstract/document/7028470
  - Kanerva 2009 (21p)
    - [[http://www.rctn.org/vs265/kanerva09-hyperdimensional.pdf][Hyperdimensional Computing: An Introduction to Computingin Distributed Representation with High-DimensionalRandom Vectors]]
      - Cogn Comput (2009) 1:139–159
      - DOI 10.1007/s12559-009-9009-8
*** Kanerva referencing papers
- [[https://arxiv.org/abs/1804.01756][The Kanerva Machine: A Generative Distributed Memory]]
  - [[https://scholar.google.com/scholar?cites=9888262262485457347&as_sdt=5,48&sciodt=0,48&hl=en][Citations as per gScholar]]
  - [ ] What's this say about KPU? is it supporting evidence?
  - Seems [[https://www.researchgate.net/publication/324246418_The_Kanerva_Machine_A_Generative_Distributed_Memory][Lillicrap uploaded it to researchgate]] and that copy says it was publish as conference paper ICLR 2018
  - arxiv
    - [[https://arxiv.org/abs/1804.01756][Overview]]
    - [[file:reading_list/kanerva_machine.pdf][cached kanerva_machine.pdf]]
**** Lillicrap
- [[http://contrastiveconvergence.net/~timothylillicrap/index.php][homepage of timothy lillicrap]]
- [[https://scholar.google.ca/citations?hl=en&user=htPVdRMAAAAJ&view_op=list_works&sortby=pubdate][On gScholar]]
- [[https://www.youtube.com/watch?v=vbvl0k-aUiE][Deep Learning and the Brain 2019 – Dr. Timothy Lillicrap]]

** TODO AMU
- [[https://twitter.com/johntigue/status/545883947431763968][Twitter(JFT, 2014)]]
- [[http://ltu.diva-portal.org/smash/record.jsf?pid=diva2%3A990549&dswid=2490][Ubiquitous Cognitive Computing: A Vector Symbolic Approach]]
  - Emruli, Blerim 
  - 2014 Doctoral thesis
  - Luleå University of Technology, Department of Computer Science, Electrical and Space Engineering, Embedded Internet Systems Lab.
** HD and VSA Workshop 2020
- [[https://sites.google.com/view/vsaworkshop2020/program][Online Webinars on Developments in Hyperdimensional Computing and Vector-Symbolic Architectures]]
*** Tony Plate
- [[https://youtu.be/6ch6fXwraZQ][Vector representations + addition + multiplication = conceptual reasoning]]
  - Reviews high-level history, CS and NS
  - Marr and Poggio's trilevel model
  - [[https://en.wikipedia.org/wiki/Metaphors_We_Live_By#:~:text=Metaphors%20We%20Live%20By%20is,time%2C%20mental%20activity%20and%20feelings.][Lakoff and Johnson's "Metaphors we live by"]] 1980
  - [[https://www.youtube.com/watch?time_continue=322&v=6ch6fXwraZQ&feature=emb_logo][Plate's Linear VSA manifesto]]
  - [[http://www2.fiit.stuba.sk/~kvasnicka/CognitiveScience/6.prednaska/plate.ieee95.pdf][Holographic Reduced Representations (1995)]]
*** Ross Gaylor
- [[https://youtu.be/mEW5HeUx504?t=226][VSA, Analogy, and Dynamic Similarity]]
  #+begin_quote
  This talk is not a presentation of any new material. It's basically
  me trying to sell an idea which i think is very important, which I've
  been working on for quite a long time but I think it would be very
  valuable if if more people would be looking at it. So obviously it's
  bound up with VSA, hyperdimensional computing but also analogy which
  is my primary motivation and I think the idea which I'd like people
  to take away is one of dynamics similarity.
  #+end_quote
- JFT
  - He's saying the the graph isomorphism must be created in realtime, not ahead of time
    - "Arguable that **representations are created on-the-fly** in response to task demands"
      - Recommends reading Chalmers, French, & Hofstadter, 1992
        - [[https://d1wqtxts1xzle7.cloudfront.net/7369801/ScienceofArt.pdf?1325334763=&response-content-disposition=inline%3B+filename%3DThe_science_of_art_A_neurological_theory.pdf&Expires=1595787260&Signature=NS9T1Tt5aM6fpxEJlNUQ8B6UBwPCle9rFNebYyXN~vix2JBb2NBja2BIwZ~Rw-1IUIvdfJOoD4mfW6ovHqOLICcrOZ5pAjLD5FOIpCMGyDaVZtBnx8UL0V0GQBsac3SBZgQiO0T2zEgfeIda~KxpgQ~txp~5wi2u9qJUdZUkxHvbo48CHxujSO5XFB6Em1zJPg24CPfU8Mh2yeyNFE4dDCJ2c3ze3CKSScZsmd8g1YQ1e3Zn4NFVP1sVZ8UZG3sgGi6BT1tQj2rs7VjQvH5-m1-gng6sczjUnk3aHEzOECqSr~ZsMyCSWAjiMQj5EjycXDP9U0EuB3MRVJGpr7Kgew__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA#page=3][High-Level Perception, Representation, and Analogy:A Critique of Artificial Intelligence Methodology]]
 
*** Paxon Frady
- [[https://youtu.be/T0mqBCpDqwk?t=21][Resonator circuits: a neural network for efficiently solving factorization problems]]

